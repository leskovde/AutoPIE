\chapter{Evaluation}\label{chap:evaluation}

Having described the implementation of proposed reduction approaches, we can 
evaluate their performance and compare them. 
The main goal of the comparison is to confirm or refute our hypothesis that 
the slicing-based approach is the most practical minimization algorithm.

\section{Metrics}\label{chap:metrics}

There are several interesting measurements to be made in this comparison.
Presentation of each algorithm in Section~\ref{chap:minimization} contained 
a description of its time complexity and its minimization properties. 
Due to this fact, we believe both the level of reduction and the algorithm's 
efficiency should be measured. 
In order to capture the performance of each approach, we employed 
the following metrics.
\begin{itemize}
\item This project focuses on minimality. 
  We want to test whether an algorithm produces the optimal result. 
  The \emph{minimality} metric is measured in two values: true and false. 
  True corresponds to the result being the desired minimal variant, while 
  false means that the result is suboptimal.
   We expect the naive and slicing-based algorithms to acquire significantly 
   more minimality than the Delta debugging approach.
  \item We have noted that some approaches, such as the minimizing Delta 
  debugging algorithm, do not achieve optimal reduction. 
  However, we can measure the distance of the generated result's size from 
  the minimal variant's size. 
  The \emph{distance from minimal variant} will be measured in percentage. 
  The goal is to achieve $0\%$, which translates to the result being 
  minimal.
  \item{Approaches} can be compared against each other using 
  a \emph{proportion ratio}. 
  This metric is also measured in percentage. 
  The number between $0\%$ and $100\%$ can be interpreted as the result's 
  proportion of the original program's size. 
  The lower the proportion ratio is, the better the algorithm is at source 
  code reduction.
  \item A straightforward way of measuring a program's performance is by 
  watching its \emph{execution time}. 
  The time will be measured in seconds and will serve as the primary 
  indicator of each algorithm's performance.
  \item The metric for testing heuristics is the \emph{processed variants} 
  count. 
  By counting how many actual results had to be generated and validated 
  before settling on the final output, we can evaluate the effectiveness of 
  a heuristic.
  The number of processed variants is compared with the expected number 
  of variants, i.e., the worst-case scenario.
\end{itemize}
By measuring these properties, we also observe whether the presented 
approaches struggle with a given input. 
Poor handling of specific inputs could also boil down to lacking 
implementation. 
However, we assume that errors in the implementation can be spotted in three 
ways. 
The program either throws an exception, produces an unreduced source code as 
its result, or outputs a program that does not result in the desired runtime 
error.

\section{Benchmarks}

Each implemented approach works with three major input parts:
\begin{itemize}
  \item It receives the source code it should minimize.
  \item It is given a location of the desired runtime error.
  \item It requires a set of arguments used when running the input source 
  code, which leads to the mentioned runtime error.
\end{itemize}
Some approaches might benefit from other user inputs. 
For example, the implementation of the naive algorithm can cut the search 
short if it exceeds a given amount of variants. 
These inputs will be referred to as minor arguments.

Our set of benchmarks consists of 30 simple programs. 
We created these programs ourselves to suit the capabilities of AutoPIE 
specifically. 
Using existing benchmarks is not viable due to the exponential time 
complexity of the naive algorithm.
The configuration for each program includes its source code written in C or 
C++, the target location, its execution arguments, and each approach's 
minor arguments. 
The set of benchmarks is made up of three equally-sized parts. 
Those parts differ based on the techniques used in their entries' source code. 
The first ten elements represent non-structured programs. 
These entries contain source code exclusively in their \icode{main} function. 
Generally, they do not follow any specific programming paradigms and 
represent the simplest input type. 
The second part contains ten structured programs. 
The source code of these programs uses control flow statements, functions, 
and procedures. 
This type of input represents the regular program that this project is 
expected to handle. 
The last ten programs use aspects of object-oriented programming. 
Their code contains classes, inheritance, and polymorphism. 
Furthermore, the code is almost exclusively written in C++. 
However, advanced features of the language, such as templates, are omitted.

All three parts of the benchmark set are similar in terms of size. 
The source code for each program ranges from 30 to 100 lines. 
Moreover, the source code is contained in a single file. 
Therefore, the programs do not use any other include headers outside of 
system headers and standard libraries. 
Runtime errors in the benchmarks are caused mainly by invaliding an assertion. 
However, segmentation faults make up a large number of errors as well. 
We have also limited each program to cause only a single runtime error 
initially.

\section{Results}

We tested a total of three approaches. 
These include the naive approach with its heuristics, the minimizing Delta 
debugging approach, and the slicer-based approach 
with argument injection. 
All experiments were executed on a 6C/12T AMD Ryzen 5 5600X processor with 
8GB of memory. 
Specifically, the chip was clocked at 4.85GHz for single-threaded tasks and 
4.5GHz for multi-threaded workloads. 
The naive approach ran parallelized on 12 threads, while other approaches 
ran single-threaded. 
The project was compiled using Clang 11.0.0 with high optimizations 
(\icode{-O2}) and ran on Ubuntu 20.04.2.0 LTS.

Table~\ref{tab:results} contains the results of all executed benchmarks. 
Each row represents a single entry from the set of benchmarks. 
The entry has been used three times: in the naive approach, the Delta 
debugging algorithm, and the slicing-based approach. 
Those 30 rows thus represent a matrix of all executed runs and their results. 
Metrics introduced in Section~\ref{chap:metrics} are denoted as follows:
\begin{itemize}
  \item The \emph{minimization ratio} is represented by the column labeled 
  with the symbol $\mathcal{R}_m$. 
  Values are in percentages.
  \item The binary \emph{minimality} metric does not have its column. 
  However, it can be derived from the column labeled with the symbol
  $\mathcal{R}_m$ by looking for rows in which the value of 
  $\mathcal{R}_m$ is zero.
  \item The \emph{proportion ratio} is represented by the column labeled 
  with the symbol $\mathcal{R}_p$. 
  Values are in percentages and correspond to the ratio of input and output 
  sizes in bytes.
  \item Run's \emph{execution time} can be found in the column labeled with 
  the symbol $\Delta_t$. 
  Time is measured in the most relevant unit based on the run's duration.
  \item The \emph{processed variants count} is in the column labeled with 
  the symbol $\mathcal{I}_a$. 
  It can be directly compared to the column labeled with the symbol 
  $\mathcal{I}_e$. 
  $\mathcal{I}_e$ represents the expected number of processed variants - 
  the worst-case scenario.
\end{itemize}

\begin{table}[b!]\centering
\begin{tabular}{rSrSSS} \toprule
	 \multicolumn{6}{c}{Naive} \\
	 \cmidrule(l){1-6}
    {$Test$} & {$\mathcal{I}_e$} & {$\mathcal{I}_{a}$}  & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} \\ \midrule
    1  & {$2^{20}$} & 15 & 0.0 & 54.870 & 3.448 \\
	2  & {$2^{40}$} & 344 & 0.0 & 31.099 & 437.648  \\
    3  & {$2^{25}$} & 1186 & 0.0 & 43.231 & 589.704 \\
    4  & {$2^{30}$} & 20 & 2.9 & 43.800 & 10.896  \\
    5  & {$2^{66}$} & 3 & 90.291 & 100.0 & 8.097  \\
    6  & {$2^{17}$} & 287 & 0.0 & 76.496 & 62.058  \\
    7  & {$2^{22}$} & 14 & 0.0 & 51.823 & 1.908  \\
    8  & {$2^{51}$} & 1079 & 40.875 & 50.714 & 907.325  \\
    9  & {$2^{29}$} & 447 & 0.0 & 58.930 & 296.069  \\
    10 & {$2^{62}$} & 2949 & 2.405 & 44.689 & 2240.359  \\ \midrule
    11 & {$2^{31}$} & 245 & 3.661 & 23.536 & 186.556  \\
    12 & {$2^{44}$} & 5 & 0.0 & 54.985 & 4.744  \\
    13 & {$2^{40}$} & 458 & 7.481 & 59.009 & 581.760  \\
    14 & {$2^{40}$} & 717 & 0.0 & 46.217 & 914.584 \\
    15 & {$2^{46}$} & 1711 & 23.196 & 58.144 & 2891.869  \\
    16 & {$2^{41}$} & 593 & 0.0 & 19.294 & 796.066  \\
    17 & {$2^{40}$} & 249 & 3.155 & 72.702 & 314.483 \\
    18 & {$2^{31}$} & 103 & 0.0 & 50.513 & 76.034  \\
    19 & {$2^{42}$} & 451 & 0.0 & 64.991 & 630.035  \\
    20 & {$2^{24}$} & 38 & 0.0 & 53.403 & 15.062  \\ \midrule
    21 & {$2^{41}$} & 592 & 28.755 & 55.043 & 792.017  \\
    22 & {$2^{41}$} & 1520 & 32.316 & 79.096 & 2038.496  \\
    23 & {$2^{50}$} & 5474 & 69.104 & 100.0 & 5131.151  \\
    24 & {$2^{32}$} & 15632 & 0.0 & 34.453 & 1280.561  \\
    25 & {$2^{35}$} & 1296 & 11.747 & 55.705 & 1267.352 \\
    26 & {$2^{35}$} & 725 & 4.874 & 65.880 & 707.148 \\
    27 & {$2^{46}$} & 19625 & 12.652 & 64.720 & 334.753 \\
    28 & {$2^{49}$} & 39588 & 51.731 & 100.0 & 7662.464 \\
    29 & {$2^{65}$} & 54061 & 57.958 & 100.0 & 1135.385 \\
    30 & {$2^{79}$} & 13775 & 73.127 & 100.0 & 6363.892 \\	\midrule
	{$Mean$} & {$~2^{40}$} & 5440.067 & 17.208 & 60.345 & 1356.330 \\	\bottomrule
\end{tabular}
\caption{Results of all benchmarks executed using the naive approach.}
\label{tab:resultsnaive}
\end{table}

\begin{table}[b!]\centering
\begin{tabular}{rSrSSS} \toprule
	 \multicolumn{6}{c}{Delta} \\
	 \cmidrule(l){1-6}
    {$Test$} & {$\mathcal{I}_e$} & {$\mathcal{I}_{a}$}  & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} \\ \midrule
    1  & {$2^{20}$} & 60 & 0.0 & 54.870 & 11.200 \\
	2  & {$2^{40}$} & 80 & 54.562 & 85.661 & 23.200  \\
    3  & {$2^{25}$} & 45 & 28.437 & 71.668 & 13.625  \\
    4  & {$2^{30}$} & 47 & 28.7 & 69.600 & 16.410  \\
    5  & {$2^{66}$} & 171 & 77.122 & 86.826 & 44.286  \\
    6  & {$2^{17}$} & 190 & 13.248 & 89.745 & 11.731  \\
    7  & {$2^{22}$} & 40 & 32.586 & 84.415 & 11.883  \\
    8  & {$2^{51}$} & 90 & 49.958 & 59.798 & 30.090  \\
    9  & {$2^{29}$} & 51 & 28.355 & 87.286 & 15.979  \\
    10 & {$2^{62}$} & 52 & 34.268 & 76.553 & 34.224  \\ \midrule
    11 & {$2^{31}$} & 92 & 29.497 & 49.373 & 18.352  \\
    12 & {$2^{44}$} & 176 & 5.865 & 60.850 & 29.744  \\
    13 & {$2^{40}$} & 88 & 38.251 & 89.779 & 23.529  \\
    14 & {$2^{40}$} & 88 & 42.491 & 65.052 & 23.527  \\
    15 & {$2^{46}$} & 78 & 30.103 & 90.188 & 26.588  \\
    16 & {$2^{41}$} & 91 & 70.893 & 88.614 & 24.231  \\
    17 & {$2^{40}$} & 85 & 19.067 & 78.974 & 23.400 \\
    18 & {$2^{31}$} & 74 & 28.462 & 87.196 & 17.794 \\
    19 & {$2^{42}$} & 78 & 22.204 & 53.403 & 24.276  \\
    20 & {$2^{24}$} & 60 & 0.0 & 41.734 & 23.400  \\ \midrule
    21 & {$2^{41}$} & 87 & 15.450 & 80.790 & 170.794  \\
    22 & {$2^{41}$} & 193 & 34.011 & 88.614 & 24.276  \\
    23 & {$2^{50}$} & 105 & 52.559 & 83.455 & 28.413  \\
    24 & {$2^{32}$} & 1146 & 19.375 & 53.828 & 30.25  \\
    25 & {$2^{35}$} & 197 & 50.232 & 91.190 & 52.672 \\
    26 & {$2^{35}$} & 87 & 23.585 & 84.591 & 24.395 \\
    27 & {$2^{46}$} & 108 & 41.365 & 93.430 & 20.545 \\
    28 & {$2^{49}$} & 108 & 45.698 & 93.966 & 27.968 \\
    29 & {$2^{65}$} & 119 & 50.964 & 93.006 & 29.792 \\
    30 & {$2^{79}$} & 200 & 67.082 & 93.955 & 40.235 \\	\midrule
	{$Mean$} & {$~2^{40}$} & 136.2 & 34.480 & 77.617 & 25.634 \\	\bottomrule
\end{tabular}
\caption{Results of all benchmarks executed using Delta debugging.}
\label{tab:resultsdelta}
\end{table}

\begin{table}[b!]\centering
\begin{tabular}{rSrSSS} \toprule
	 \multicolumn{6}{c}{Slicing} \\
	 \cmidrule(l){1-6}
    {$Test$} & {$\mathcal{I}_e$} & {$\mathcal{I}_{a}$}  & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} \\ \midrule
    1  & {$2^{20}$} & 15 & 0.0 & 54.870 & 14.690 \\
	2  & {$2^{40}$} & 57 & 0.0 & 31.099 & 116.531  \\
    3  & {$2^{25}$} & 17 & 0.0 & 43.231 & 134.972  \\
    4  & {$2^{30}$} & 207 & 2.9 & 43.800 & 22.691  \\
    5  & {$2^{66}$} & 263 & 0.0 & 9.708 & 56.977  \\
    6  & {$2^{17}$} & 41 & 0.0 & 76.496 & 27.074  \\
    7  & {$2^{22}$} & 4 & 0.0 & 51.823 & 15.232  \\
    8  & {$2^{51}$} & 85 & 0.0 & 9.840 & 219.078  \\
    9  & {$2^{29}$} & 21 & 0.0 & 58.930 & 79.188  \\
    10 & {$2^{62}$} & 82 & 2.004 & 44.286 & 490.852 \\ \midrule
    11 & {$2^{31}$} & 192 & 0.837 & 20.711 & 60.251  \\
    12 & {$2^{44}$} & 72 & 0.0 & 54.985 & 38.129  \\
    13 & {$2^{40}$} & 1035 & 0.0 & 51.528 & 145.752  \\
    14 & {$2^{40}$} & 1040 & 0.0 & 46.217 & 212.317  \\
    15 & {$2^{46}$} & 56678 & 0.0 & 34.949 & 611.609  \\
    16 & {$2^{41}$} & 30 & 0.0 & 19.294 & 189.502  \\
    17 & {$2^{40}$} & 9482 & 0.0 & 69.547 & 92.146 \\
    18 & {$2^{31}$} & 12 & 0.0 & 50.513 & 374.449  \\
    19 & {$2^{42}$} & 1067 & 0.0 & 64.991 & 156.352  \\
    20 & {$2^{24}$} & 6 & 0.0 & 53.403 & 198.125  \\ \midrule
    21 & {$2^{41}$} & 807 & 15.451 & 41.738 & 188.487  \\
    22 & {$2^{41}$} & 490 & 32.316 & 79.096 & 443.215  \\
    23 & {$2^{50}$} & 1450 & 52.559 & 83.455 & 1064.043  \\
    24 & {$2^{32}$} & 992 & 0.0 & 34.453 & 321.952  \\
    25 & {$2^{35}$} & 1210 & 11.747 & 52.705 & 283.964 \\
    26 & {$2^{35}$} & 123 & 20.597 & 81.604 & 167.111 \\
    27 & {$2^{46}$} & 28232 & 12.652 & 964.720 & 703.516 \\
    28 & {$2^{49}$} & 7804 & 45.698 & 93.966 & 1569.733 \\
    29 & {$2^{65}$} & 80387 & 50.965 & 93.006 & 2773.706 \\
    30 & {$2^{79}$} & 53834 & 67.083 & 93.955 & 1339.278 \\	\midrule
	{$Mean$} & {$~2^{40}$} & 8391.167 & 10.493 & 53.631 & 303.309 \\	\bottomrule
\end{tabular}
\caption{Results of all benchmarks executed using the slicing pipeline.}
\label{tab:resultsslicing}
\end{table}

\begin{table}[b!]\centering
\begin{tabular}{rSSSSSSSSS} \toprule
	 & \multicolumn{3}{c}{Naive} & \multicolumn{3}{c}{Delta} & \multicolumn{3}{c}{Slicing} \\
	 \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10}
    {$Test$} & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} & {$\mathcal{R}_{m}$} & {$\mathcal{R}_{p}$} & {$\Delta_{t}$} \\ \midrule
    1  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    2  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    3  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    4  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    5  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    6  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    7  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    8  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    9  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    10  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\ \midrule
    11  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    12  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    13  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    14  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    15  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    16  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    17  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    18  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    19  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    20  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\ \midrule
    21  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    22  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    23  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    24  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    25  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    26  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    27  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    28  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    29  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\
    30  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\ \midrule
	{$Mean$}  & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s & 0.0 & 0.0 & 0.0s \\	\bottomrule
\end{tabular}
\caption{Results of all benchmarks executed using the slicing pipeline.}
\label{tab:resultsslicing}
\end{table}

The table's test numbering is systematic. 
The first ten rows in the table correspond to the unstructured portion of 
the benchmark. 
They are meant to make the most effective use of every proposed approach. 
These test programs are written exclusively in C. 
Tests 11 to 20 represent the structured part of the benchmark. 
These programs define several functions and call them inside 
the \icode{main} function. 
This portion consists of both C and C++ programs.
The ten remaining rows numbered 21 to 30 correspond to the OOP part of 
the benchmark. 
These tests are almost solely written in C++, and they test the reduction 
of classes, structures, and their respective members.
The numbering is not sorted by any means.

The \emph{reduction ratio} is calculated using the raw input and output 
files. 
These files contain explanatory comments, and thus their size might be 
inflated. 
For example, a reduction from 953 bytes to 445 bytes only results in a ratio 
of $46.7\%$. 
One might think that this ratio is not impressive. 
However, the output removed all but three statements. 
The 445-byte large output mainly consists of comments leftover from the input.

The \emph{minimization ratio} is determined based on a handcrafted minimal 
program variant. 
We checked the optimal solution for each element in the set of benchmarks and 
compared the output to that solution. 
Although it was just stated that the optimal reference variant is minimal, 
it is not entirely true. 
This project does not employ advanced techniques such as argument removal 
and function body substitution. 
Therefore, it cannot produce the optimal variant in terms of language 
constructs and raw byte size. 
Instead, we considered the best possible solution this project can achieve, 
knowing its limitations. 
This consideration leads to the fact that the minimal reference variant 
might be larger than the optimal solution since it might contain additional 
arguments or required function definitions.

The measured \emph{execution time} is based on the \icode{time(1)} Unix shell 
utility readings. 
Values in the table match the wall clock time reported by the utility. 
While the time was being measured for a given minimization approach, we 
executed the approach in its most efficient configuration. 
In other words, we disabled all logging and verbose options, omitted 
the GraphViz visualization, and set appropriate reduction ratios for 
the naive approach. 
Each execution of the naive approach ran with its bin heuristic, and we 
opted for five bins for every run. 

This method of measuring can be unfair to the slicing-based approach. 
Its implementation launches multiple Docker containers. 
Not only does the code in those containers run slower than native code, 
but running containers also introduces a delay in the processing. 
Small inputs suffer from this issue the most since the constant delay stacks 
up, outweighing the time spent running the minimizing techniques.

\section{Limitations}\label{chap:limitations}

The implementation described in Section~\ref{chap:implementation} has its 
pitfalls. 
While the proposed algorithms might work with any project in the domain 
described in Section~\ref{chap:minimization}, the implementation cuts corners 
and further narrows down the supported input. 
The following paragraphs describe the challenges we faced and how they have 
impacted the capabilities of our prototype implementation of the proposed 
approaches.

\paragraph{Input reading at runtime.}
A problem that immediately comes to mind is getting user input from 
the standard input. 
Generally, we could simulate the input in two ways. 
First, we could ask the user for a list of all entered values during 
the user's run. 
The list would then be provided to the program during validation. 
The second option is always to assume the worst possible outcome. 
Places in the code that expect an input would be supplied with 
the worst-case input. 
However, it should be noted that the latter approach does not consistently 
trigger the desired behavior.

While this paragraph serves as a proof of concept for handling standard 
input, we decided not to implement any handlers. 
Instead, processing programs that utilize the standard input will result in 
undefined behavior.


\paragraph{Variables in namespaces.}
Used slicer implementations have their interfaces. 
In particular, DG's variable criterion interface requires the user to enter 
a criterion in the form of \icode{line:variable}. 
Since the criterion contains a colon character, it cannot parse variable 
names that contain namespaces. 
We do not explicitly handle this case and instead deem this kind of input as 
unsupported.

\paragraph{Non-optimal minimal solutions.}
Due to the enormous scope of this project, we could not implement advanced 
and more granular techniques for code reduction. 
Notably, we do not remove function arguments, constructor initializers, and 
control flow statements. 
The last is due to semantical reasons. 
We also do not substitute function bodies. 
The lack of these techniques leads to sub-optimal results in more 
complicated code. 
The implemented approaches lack the granularity to reduce the sub-optimal 
solution further. 

Additionally, we believe that the reduction should preserve as much 
readability as possible. 
Due to this belief, we do not rename variables, functions, and classes to 
shorter names.

\paragraph{Multiple files as the input.}
While the pseudocode shown in Section~\ref{chap:minimization} does not look 
at the input as a set of multiple files, we can assume that a correct 
implementation can handle multiple input files without any issues. 
However, we have decided against supporting the following: header file 
reduction and multiple source file reduction. 
Instead, our current prototype implementation can only handle a single input
file.
The file must not be header, although it can contain includes for header 
files. 
Those header files will not be reduced. 
Moreover, any included non-standard header files must be available at their 
presumed locations for correct parsing and compilation.

The decision to support only a single input file comes from 
the complications implicated by non-trivial compilation. 
Section~\ref{chap:validationimplementation} describes how generated reduced 
program variants are validated. 
The validation compiles the project, and any complicated project would 
require a custom compilation command. 
In order to simplify the matter, in the current prototype we only support 
single-file projects that can be trivially compiled into an executable 
binary.

\paragraph{Slicer inaccuracy.}
As was already mentioned, the used slicer implementations have their 
pitfalls. 
However, even in the best-case scenarios, they still might provide us with 
poor output. 
Most notably, the conversion from LLVM bitcode to a list of line numbers is 
not always accurate. 
Often, the result is incomplete, and we have to perform additional steps to 
get a syntactically correct slice. 
For example, variables with the *const* qualifier might be absent from 
the slice even though their values are necessary for parts of the slice. 
We have tacked and fixed most of these inaccurate cases. 
Still, there remain inaccuracies in the slices. 
For example, the slicer result might contain a line number greater than 
the length of the sliced source file. 
We are aware of this and other problems surrounding the used implementations. 
Due to this project's scope, we decided to ignore inaccurate slices and 
replace them with the original input file if needed.

\paragraph{LLDB symbol inconsistency.}
The use of LLDB is the reason why our implementation does not run correctly 
on Windows. 
LLDB suffers from a couple of issues - in the mentioned Windows case, it does 
not load files with debugging symbols.
A significant problem we encountered while working with LLDB is 
the inconsistency of symbol locations. 

During validation, we found out that an error's location might vary from run 
to run. 
When considering the top-most frame on the stack, LLDB might read its 
location correctly. 
The source file variant being validated can then be rightly processed further. 
However, LLDB often misinterprets the frame's location and instead reports 
a line number we might consider invalid. 
The line number often corresponds to the declaration of the current function 
or the function's opening or closing curly braces. 
We have tried our best to capture these locations during the initial parsing 
and later handle them correctly. 
Nonetheless, we are aware of the fact that the modified validation might 
result in an irregular result.