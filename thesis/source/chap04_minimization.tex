\chapter{Program minimization}\label{chap:minimization}

As described in the first chapter, debugging is a time-consuming task.
Any amount of help with debugging is always appreciated by developers.
In this project, we attempt to help by providing means to minimize the 
debugged program w.r.t. a given runtime error.
The minimization's goal is to reduce the amount of source code programmers 
must go through when debugging, thus speeding up the process.
The size reduction of the program should be fully automated and reasonably 
fast on simple inputs.
Furthermore, it should correctly handle any source code from the program 
domain specified below.
Great attention is given to the accuracy with which the minimizing algorithms 
work and their time efficiency.

The domain in which the algorithms that are shown below operate can be 
described as small and simple projects.
The presented approaches take into consideration code written in C and C++.
Support for more complicated concepts of those languages, such as templates,
is omitted.
Additionally, programs that involve multiple threads and other advanced
features that might trigger non-deterministic behaviour are also not taken 
into consideration.
Programs that rely on randomly generated numbers during their runtime do not 
fit into the domain as well. 
This is because executions of multithreaded and random programs cannot be 
easily reproduced.
Instead, the program minimization described in this project focuses 
on simple single-threaded console applications with consistent executions.

The problem of program minimization while preserving runtime errors can be 
described as follows.
Assume that a developer has encountered a runtime error in his application.
Using logging or debugging tools, he can extract the stack trace at that 
given point.
The stack trace provides valuable information for a minimizing algorithm.
The presented algorithms notably require a description of the error and 
the source code location at which the error was produced.
Based on the described scenario, we can draw the following definitions.

\begin{defn}[Location]\label{def04:1}
  Let $loc$: $\mathcal{S} \mapsto \{x | x = (file, line, col)\}$, 
  where $\mathcal{S} = \{S_1, S_2, \ldots, S_n\}$ 
  is the set of program's statements.
  We call the result of $loc(S_i)$ the \emph{location} of statement $S_i$.
\end{defn}

The source code's location is specified by a file name, the line number, and 
on that line, the number of characters from the left.
The location could be described in further detail by including starting and 
ending points.
However, in this simplified description, only the starting point is taken 
into consideration.

\begin{defn}[Failure-inducing statement]\label{def04:2}
  Let $E = (location, desc)$ be a runtime error specified by its location 
  and description. Let the program $\mathcal{P} = (S_1, S_2, \ldots, S_n)$
  result in $E$ upon execution.
  We call $S_i$ the \emph{failure-inducing statement} of 
  $E$ if $loc(S_i) = E(location)$.
\end{defn}

Failure-inducing statements are the sites from which an error was thrown.
That means the statements were present at the error's location when the error 
occurred.

Having found the site's source code location, the developer can now 
investigate the source code for a potential bug.
In the process, he might consider the values of application arguments 
present at launch-time and change his debugging process accordingly.
Nonetheless, the developer has to look through the source code to find 
the error's root cause.
This exact point is where the source code size reduction starts being 
beneficial.
Using static and dynamic analysis, it is possible to effectively and 
safely remove unnecessary source code.
Such code includes statements, declarations, and expressions that do not 
affect the program's state at the point given by the error.
With additional verification, it is also possible to remove code constructs 
that affect the state, but the error occurs regardless of whether they 
are present or not.

The reduced program's source code can then be used for debugging the given
runtime error.
The newly generated program has to fulfill the following invariant.

\begin{invar}[Location alignment]\label{invar04:1}
  Every program $\mathcal{P'}$ created by reducing the original program 
  $\mathcal{P}$ based on dynamic information given by the execution of
  $\mathcal{P}$ with arguments $A$ must result in the same runtime error $E$.
  The error's absolute location can differ; it must, however, occur in the 
  same context.
\end{invar}

The rule specifies that a program must end in the same runtime error 
as the original program to be considered a correctly reduced variant.
Though, with the change to the program's size, the location 
of failure-inducing statements also changes.
In $\mathcal{P}$, the error's location in the file should be lower compared 
to $\mathcal{P'}$ since $\mathcal{P'}$ has less code in general.
Stress is placed on the location's context in which the error arises.
As long as locations in $\mathcal{P'}$ are adjusted based on those in 
$\mathcal{P}$, the absolute location of the error does not matter.

Figure~\ref{lst:reductionexample} contains an example of $\mathcal{P}$ and its 
minimal variant $\mathcal{P'}$. 
All statements that do not directly contribute to the specified 
runtime error are removed.
A non-minimal variant might contain additional non-impactful lines, 
such as line 32.

\begin{figure}[p]
\begin{minipage}{0.46\textwidth}
\begin{lstlisting}[basicstyle=\small, caption=Program $\mathcal{P}$.,
  language=C++, label={lst:reductionoriginal}]
#include <stdio.h>
#include <stdlib.h>

long get_factorial(int n)
{
	// Missing the stopping 
	// constraint 
	// => segmentation fault.
	return (n * 
		get_factorial(n - 1));
}

int main()
{
	const int n = 20;
	long loop_result = 1;
	
	for (int i = 1; i <= n; 
		i++)
	{
		loop_result *= i;
	}
	
	long recursive_result = 
		get_factorial(n);
	
	if (loop_result != 
		recursive_result)
	{
		printf("%ld, %ld\n", 
			loop_result, 
			recursive_result);
			
		return (1);
	}
	
	printf("Success.\n");
	
	return (0);
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[basicstyle=\small, caption=Minimal va\-riant $\mathcal{P'}$., 
  language=C++, numbers=right,
  label={lst:reductionminimal}]
#include <stdio.h>
#include <stdlib.h>

long get_factorial(int n)
{
	// Missing the stopping 
	// constraint 
	// => segmentation fault.
	return (n * 
		get_factorial(n - 1));
}

int main()
{
	const int n = 20;
	
	
	
	
	
	
	
	
	long recursive_result = 
		get_factorial(n);
	
	
	
	
	
	
	
	
	
	
	
	
	
	
}
\end{lstlisting}
\end{minipage}
\caption{A program resulting in a segmentation fault error and its 
minimal erroneous variant. 
The variant is stripped off all statements unnecessary for the error 
to occur. }
\label{lst:reductionexample}
\end{figure}

\change[inline]{TODO: Come up with an actual way to make sure a program 
is minimal.}
\change[inline]{TODO: Come up with an approximation to guess whether
the program will terminate.}

So far, this chapter has talked about both minimization and reduction 
simultaneously. 
It is crucial to make a distinction between those two terms. 
In this context, the reduction is simply the process of making the program 
smaller in size. 
The reduced program must also result in the same source code. 
Minimization is built on the same rules as reduction; however, it must 
fulfill one additional property. 
No statement of the minimized program can be removed while preserving 
the error. 
The task of reduction is more straightforward than that of minimization. 
Finding the program's minimal variant is computationally infeasible for 
large inputs. 
Whereas reducing the program to a rough approximation of the optimal 
solution can be done in polynomial time. 
Although this project focuses on minimal program variants, we recognize that 
it is expensive to compute them. 
Instead, we use reduction and minimization interchangeably throughout 
the text.

Minimization of programs requires two stepsâ€”first, the removal of chunks 
of the given source code. 
The following sections describe several techniques of code removal. 
The naive approach is explained briefly. 
Possible improvements to that approach concerning runtime are then described. 
Subsequent approaches employ techniques discussed in 
chapter~\ref{chap:automated}. 
The method based on Delta debugging offers a modified version of 
the debugging algorithm. 
Another approach combines different types of slicing to achieve the best 
results.

Second, the minimization needs to perform a validation to determine whether 
the result meets the required criteria, i.e., minimality (or an approximation) 
and correctness.
The description of naive validation is shown in the sections below.

\section{Naive reduction}\label{chap:naive}

The simplest approach examined in this project is the naive removal of each 
source code statement.
This technique aims to try every possible variation of the code and find 
the smallest correct solution through trial and error.
All possible variations, both valid and invalid at compile-time, can be 
generated by separating the source code into units of statements, 
declarations, and expressions and removing one code unit at a time.

\begin{defn}[Code unit]\label{def04:3}
  Let $\mathcal{P}$ be a program consisting of a sequence of statements, 
  expressions, and declarations $(S_1, S_2, \ldots, S_n)$. 
  We call $U_i = (S_{i_1},\-S_{i_2},\-\ldots,\-S_{i_n}), 
  U_i \subseteq \mathcal{P}$ a \emph{code unit} if the sequence 
  $(S_{i_1}, S_{i_2}, \ldots, S_{i_n})$ is syntactically
  correct.
\end{defn}

Algorithm in figure~\ref{alg:naive} describes the naive process. 
Once the input source code is provided, it is split into $n$ code units.
Every unit has two possible states: it is either kept or removed.
Analogically to generating every subset of a set of elements, this
variant generating algoritm results in $2^n$ possible variants.

\begin{figure}[h]
	\hrule height.8pt depth0pt \kern2pt
	\textbf{Input:} \\
	\hspace*{\algorithmicindent} $L \ldots$ location of the error. \\
	\hspace*{\algorithmicindent} $P \ldots$ the input source code. \\
	\hspace*{\algorithmicindent} $A \ldots$ the input program's arguments. \\
	\textbf{Output:} The reduced source code. 
	\hrule height.8pt depth0pt \kern2pt
	\begin{algorithmic}[1]
		\State $allVariants \leftarrow \{\}$
		\State $(C_1, C_2, \ldots, C_n) \leftarrow$ SplitIntoCodeUnits($currentVariant$)
		\State $bitField \leftarrow [{bit}_{n-1}, {bit}_{n-2}, \ldots, {bit}_0]$,  
			$\forall i \in 0 \ldots n-1 : bit_i = 0$
		\While{$\exists i \in 0 \ldots n-1 : bit_i = 0$}
			\State $currentVariant \leftarrow (C_1, C_2, \ldots, C_n)$
			\For{$i \in 0 \ldots n-1$}
				\If{bitField[i] = 0}
					\State $currentVariant \leftarrow currentVariant \setminus \{C_{i+1}\}$
				\EndIf
			\EndFor
			\State $allVariants.Add(currentVariant)$
		\EndWhile
		\State $allVariants \leftarrow$ SortBySize($allVariants$, $Ascending$)
		\ForAll{$V \in allVariants$}
			\If{IsValid($V$, $L$, $A$)}
				\Return $V$.
			\EndIf
		\EndFor
		\State \Return none.
	\end{algorithmic} 
	\hrule height.8pt depth0pt \kern2pt
	\caption{Naive Statement Removal.} 
	\label{alg:naive}
\end{figure}

The naive time complexity is, therefore, the abysmal $\mathcal{O}(2^n)$.
Moreover, the $2^n$ variants require some verification and classification 
to determine whether they are minimal or not.
The correctness of many of the invalid variants can be ruled out at 
compile-time.
The rest, however, must be executed and tested for runtime errors.

An efficient way of finding the smallest correct program variant is to rule 
out programs with compile-time errors, sort the remaining variants by size 
and verify them from the smallest to the largest.
Depending on the input program's execution time, the verification might take 
more time than the already long variant generating step.

The algorithm can be sped up by using the following techniques.
It is important to keep the input's size as low as possible due to the 
algoritm's exponential complexity.
The input's size can be significantly reduced by performing static 
slicing of the input program as a preprocessing step.
Compared to additional iterations of the naive approach, static slicing is 
a significantly less expensive operation in terms of running time.
Furthermore, it does not execute the input program, saving additional time.
Out of the two main issues concerning the current discussed approach - 
the size of the input and its execution time - static slicing helps 
to eliminate one.

Both the input size and subsequent execution time could both be brought 
down by using dynamic slicing.
The usage of dynamic slices, as opposed to static, has its potential benefits.
On the other hand, it has definitive limitations.
One such constraint is the requirement to run the said program.
This point will be discussed in more detail later; however, let us consider 
static slicing for this approach to minimize the number of program executions.
Since static slicing does not handle branching and other control statements 
nearly as efficiently as dynamic slicing, we can employ a simple trick 
to help.
Using the same additional input information as dynamic slicing, i.e., 
program arguments, we can provide more specific information 
to the static slicing algorithm.
All that is required is to define the arguments with their respective 
values inside the code.
Slices generated from this modified source code will be more precise 
since they will not contain unnecessary branching.
It is important to restate that this modification only affects control 
statements dependent on the program's arguments.
If the arguments do not appear in the original, unmodified static slice, 
their values will not affect the slice's size.
Input modified using the mentioned trick is guaranteed to be smaller or 
equal in size.

Furthermore, removing only a single code unit, i.e., a statement, 
an expression, or a declaration, and leaving its complement sometimes 
generates an invalid program variant.
This problem can arise when removing a variable declaration.
Therefore, the removal of some code units should also remove their potential 
dependencies.
This case does not only concern the mentioned variable declarations but 
function declaration, function definitions, and struct, enum, 
and class declarations as well.

These proposed modifications can potentially reduce the number of iterations.
However, lower runtime complexity is not guaranteed.
Static slicing may help in programs whose intent is to perform multiple 
independent tasks, but it might not contribute anything to the complexity 
decrease otherwise.
The removal of dependent constructs described in the second point should be 
used regardless of the input's nature, though it still does not ensure 
the algorithms' quicker running time for every input.
The running time will especially be left unchanged in programs that do not 
employ structured or object-oriented programming.

\section{Delta debugging}\label{chap:deltaimplementation}

Zeller's Delta debugging \cite{Zeller99, Zeller02, Zeller01} has been 
described in detail in section~\ref{chap:delta}.
It is an automated debugging technique that focuses on input size reduction.
For a given input, Delta debugging attempts to find the input's 
smallest failure-inducing subset and isolate the a failure-inducing element 
using two algorithms.
This project is only concerned with the minimizing algorithm described in
section~\ref{chap:delta} and in the figure~\ref{alg:dd}.

For the rest of this section, the input will be referred to as 
a \emph{test case}.
Other than the test case, Delta debugging also requires the debugged program 
(in its executable form) and a method of validating its output.
Let us draw parallels between the mentioned requirements and this project's 
minimization task.
The input for program minimization is the given program's source code.
The code can be labeled as the test case Delta debugging takes.
It is essential to clarify that this Delta debugging usage does not utilize 
the source code as the debugged program.
Instead, it considers it as a given test case.
Then, we must specify the expected output and a method to validate it.
It is required that the program terminates with a given runtime error.
Let us label that runtime error and its location as the expected output.
Whenever the executed test case results in that particular error, we 
interpret the run as a failing one.
Every other terminating run will be interpreted as a passing one.
Lastly, we must define the debugged program.
In our case, to get from the test case (source code) to the expected 
output (a specific runtime error), the code must first be compiled 
and then executed.
The fitting debugged program is, therefore, a pipeline of a compiler and 
an execution environment.

The minimizing algorithm uses binary search in a greedy manner.
We have modified the way binary search is performed to better fit the input's
structure.
The original minimizing algorithm operates with partitions of equal size.
That is not necessarily the best approach for structured test cases such
as source code.
To give a concrete example, we can look at splitting a function definition
into two partitions.
Originally, we would get two syntactically invalid code snippets.
One would contain the function's head and the first half of its body.
The body would not contain the terminating curly bracket.
Similarly, the second would be missing an opening curly bracket.
Instead, it makes more sense to operate on code units.
A more detailed description of code units can be found in 
section~\ref{chap:naive}.

Initially, the test case is split into $k$ code units.
Those code units are then assigned into $n$ partitions of roughly the same 
size.
The number of partitions changes based on the current iteration, as described
in figure~\ref{alg:dd}.
In our case, each iteration compiles current test case subsets and executes 
them.
Executions are validated as described earlier and the algorithm reduces
the size of the test case gradually.
The result is roughly what we need - a minimal program variant approximation 
that fails with a particular error.
As was already mentioned in this chapter's introduction, the location of 
the error might differ based on the variant's structure.
Nonetheless, the location could be aligned based on the source code in 
an additional step, so that Invariant~\ref{invar04:1} holds.

The running time complexity of this modified algorithm, which is measured 
for the number of executed validations $v$, remains unchanged. 
Let us assume the test case consists of $k$ code units. 
Zeller and Hildebrandt\citep*{Zeller02} presented both the worst 
and the best case complexity as follows.

\paragraph{Worst case.} Two possible scenarios lead to the worst time 
complexity. 
First, every executed validation is inconclusive. 
That would lead to $v = 2 + 4 + 8 + \ldots + 2 * k = 4 * k$ validations. 
Second, the validation succeeds, i.e., finds a failing subset, for every 
last complement. 
This case given us $v = (k - 1) + (k - 2) + \ldots + 2 = k^2 - k$ validations.
Combined, these two scenarios lead to $k^2 + 3 * k$ validations at worst.

\paragraph{Best case.} The best case is the ideal scenario for utilizing 
binary search. 
We would be searching for a single failure-inducing code unit. 
This scenario leads to $2 * \log{}k$ validations.

Extracting an approximation of the minimal program variant after 
$\mathcal{O}(k^2)$ validations is undoubtedly practical. 
Especially when compared to the exponential time complexity of the naive 
approach described in section~\ref{chap:naive}. 
Those who desire the minimal variant might want to get the approximation 
first and provide it to the naive reduction. 
Nevertheless, there is no way of avoiding the exponential complexity when 
searching for optimal results.

\section{Slicing-based solution}\label{chap:systematic}

As mentioned in section~\ref{chap:naive}, both static and dynamic slicing 
need to be analyzed further.
The main focus of the discussion should be the running time.
It is known that dynamic slices are the smallest they can be.
However, they require information available at execution time.
The question is whether running the program is necessary.

We know that the program that is being minimized has been run before.
Hence the availability of the information about the encountered runtime 
error.
If the program ran deterministically, it would have to terminate in future 
executions as well.
That is considering it would run with the same arguments as previously.
We can therefore conclude that the program terminates.
The time of the termination might vary depending on the purpose of 
the program.
For server-like applications, it might take months to encounter an error 
at runtime.
Static slicing does not suffer from the mentioned issue.
It is inexpensive in terms of execution time regardless of the purpose 
of the sliced program.
With the arguments trick mentioned in chapter~\ref{chap:naive}, 
static slices can be as small as dynamic ones.
Minimality of the slice is, however, not guaranteed.

It is safer to employ dynamic slicing to get the smallest slices possible.
However, one can perform preprocessing steps to help dynamic slicing run more 
efficiently.
Let us consider a program that performs multiple demanding tasks 
such as computations.
These tasks are primarily independent, and their running time is long.
Using dynamic slicing alone would be inconvenient.
However, by first employing static slicing to remove these long-running 
unnecessary tasks, the program's execution time can be significantly reduced.
The reduced program could then be sliced dynamically.
The result would be a minimal slice at a fraction of the original time 
compared to dynamic slicing alone.

This crafted ideal use case only concerns a very narrow range of existing 
programs.
However, due to its low running time, static slicing could be used before 
just about any attempt at dynamic slicing.

\change[inline]{TODO: Add references to the halting problem and 
Rice's theorem.}

The improvement in the form of a static slice is genuinely convenient.
However, checking whether the improvement has any effect before running 
dynamic slicing is not an easy task.
The issue stems from the Halting problem and Rice's theorem.
The halting problem states that it is undecidable whether a program 
terminates on its particular input.
Rice expanded the thought further by stating that all interesting semantic 
properties of a program are undecidable.
Without proper and accurate means to determine many wanted properties, 
we are required to approximate them.

Amongst such properties is the factor of how effective static slicing is.
The approximation will be required in the following sections as well.
In particular, the section concerning program validation will look at this 
issue in more detail.
One way of guessing the effectiveness of static slicing in terms of size 
reduction is by analyzing the program's branching factor.
By employing a metric for the number and density of control-flow altering 
statements, we can approximate static slicing's relative performance.
It is assumed that programs with a high branching factor, i.e., with more 
control-flow-altering statements, are less likely to reduce their size 
during static slicing.
Nonetheless, slicing statically before doing so dynamically has been 
a rule of thumb for this project.

\begin{figure}[h]
	\hrule height.8pt depth0pt \kern2pt
	\textbf{Input:} \\
	\hspace*{\algorithmicindent} $L \ldots$ location of the error. \\
	\hspace*{\algorithmicindent} $P \ldots$ the input source code. \\
	\hspace*{\algorithmicindent} $A \ldots$ the input program's arguments. \\
	\textbf{Output:} The reduced source code. 
	\hrule height.8pt depth0pt \kern2pt
	\begin{algorithmic}[1]
		\State $S \leftarrow$ GetStatementAtLocation($L$)
		\State $variableList \leftarrow \{\}$
		\ForAll{$Expr \in S$}
			\If{$Expr$ is Variable}
				\State $variableList.Add(Expr)$
			\EndIf
		\EndFor
		\State $sliceList \leftarrow \{\}$
		\ForAll{$V \in variableList$}
			\State $sliceList.Add$(StaticSlice($P$, $L$, $V$))
		\EndFor
		\State $unifiedSlice \leftarrow$ Unify($sliceList$)
		\State $P' \leftarrow$ Compile($unifiedSlice$)
		\State $L' \leftarrow$ AdjustLocation($P$, $P'$, $L$)
		\State $sliceList \leftarrow \{\}$
		\ForAll{$V \in variableList$}
			\State $sliceList.Add$(DynamicSlice($P'$, $L'$, $V$, $A$))
		\EndFor
		\State $unifiedSlice \leftarrow$ Unify($sliceList$)
		\State $P' \leftarrow$ Compile($unifiedSlice$)
		\State $L' \leftarrow$ AdjustLocation($P$, $P'$, $L$)
		\State $P' \leftarrow$ PreciseReduction($P'$, $L'$, $A$)
		\State \Return $P'$.
	\end{algorithmic} 
	\hrule height.8pt depth0pt \kern2pt
	\caption{Minimization Based on Slicing.} 
	\label{alg:slicing}
\end{figure}

The proposed systematical solution is described in figure~\ref{alg:slicing}.
The input program is sliced statically w.r.t.
every variable available at the failure-inducing line.
The slices are then unified and given as the input to a dynamic slicer.
Similarly, the dynamic slicer generates slices w.r.t.
those potentially failure-inducing variables.
Those dynamic slices are then unified.
The intermediate result extracted after performing the two slicing types 
should be significantly smaller than the original program.
It is then fed to a more precise and less efficient algorithm for further 
reduction.
Since the result so far contains slices for multiple variables, it might not 
be minimal yet.
However, it can be assumed that it is valid, i.e., ends with the desired 
runtime error.
We could implement the naive approach or Delta debugging as the more 
precise algorithm.

\change[inline]{TODO: If a more sophisticated final algorithm is 
created, mention it.}

Another thought-about approach is hybrid slicing.\change{TODO: Get more information on hybrid slicing.}
The comparison of hybrid slicing and the combination of static and dynamic 
could yield exciting results.
It can be assumed that hybrid slicing would be more effective on smaller 
programs with a short execution time.
The static-dynamic combination could work better on larger-scale 
applications, where static slicing can remove unnecessarily long-running 
chunks of code.


\section{Program verification}\label{chap:verification}

\change[inline]{TODO: Add https://youtu.be/UcxF6CVueDM?t=177 as a reference.}


\change[inline]{TODO: Meditate at the thought of using slicing 
for systematic validation.
For example, a slice of the original program and the reduced program's 
slice must not differ in some parts.}

One way of achieving systematic validation is by inserting 
instrumentation code at compile-time.\change{TODO: Verify this statement and explain further.}

