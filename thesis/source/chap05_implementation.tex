\chapter{Implementation}\label{chap:implementation}

\change[inline]{TODO: Mention that the location in implementation does 
not use columns, since the presumed location in LibTooling is not
precise.}

The problem analysis in Section~\ref{chap:minimization} established several 
approaches to program reduction and minimization. 
The next step is to compare the described algorithms and techniques. 
Nevertheless, first, we must settle on their concrete implementations. 
More straightforward approaches such as the naive reduction and 
the minimizing Delta debugging algorithm deserve their implementation. 
However, implementing more complicated techniques - static and dynamic 
slicing - is simply out of scope for this project.
This chapter will explain the structure of the project and the implementation 
process of the naive reduction and the minimizing Delta debugging algorithm. 
Moreover, it will introduce the reader to all used technologies and existing 
implementations. 
Each implementation will be described in a high-level overview while 
highlighting the necessary steps for its inclusion in this project. 
The chapter concludes with a discussion on some of the design choices and 
the project's limitations.

\section{Technologies}

This project heavily depends on existing compiler and debugger frameworks. 
Section~\ref{chap:tools} has compared these frameworks and chose the ideal 
candidate for the project. 
That ideal candidate is Clang's LibTooling. 
Section~\ref{chap:libtooling} described in detail the choice of AST as 
the de facto representation of this project's input. 
LibTooling offers multiple ways of traversing and modifying the AST. 
Additionally, it does so for both C and C++.

LibTooling can be built from the LLVM repository together with Clang.
The required versions of Clang (11.0.0) and LibTooling are built from LLVM 
version 11.0.0.
Building LLVM from source is a time-consuming process that does not always 
end in the desired result.
The user must specify all required projects in advance using CMake's options.
LLVM and its projects are then built using a different build tool such as 
ninja or make.
Even though the building process can run multiple jobs at once, it can still 
take up to several hours, consuming a significant amount of the system's 
memory.
The debug build utilizes tens of gigabytes of disk space.
Thankfully, debugging symbols are not required for this project.

Clang is not the only LLVM project required as a prerequisite.
Section~\ref{chap:verification} described the steps of naive validation. 
The final step requires a sort of execution environment. 
One proposed environment is a debugger. 
In order to keep the setup process as simple as possible, we settled on LLDB. 
LLDB is an open-source debugger that ships as a part of the LLVM project.
The user needs to build LLDB with its scripting bridge API.
This can be achieved by adding LLDB to the LLVM project list when invoking 
CMake.
The Python API and its C++ scripting bridge can also be included by 
specifying a few other arguments.
By default, LLVM builds for all available platforms, including ARM and 
PowerPC.
However, only a single platform is required/supported for this project.
The target platform with which LLVM should be built is x86 64 bits.

LibTooling changes with every release.
Projects dependent on an older version of LibTooling might not work with 
a newer one.
Moreover, older releases of LLVM cannot always be built on new platforms.
From experience, the issue might arise when an old LLVM version attempts 
to link new system headers and libraries.
An easy and reliable way of preserving older LibTooling environments is by 
storing them in a Docker container.

Docker is another dependency of this project.
It is required to run slicing implementations as well as support the entire 
minimization process on Windows.

\section{Design}

Since this project compares several techniques, some of which are composed in 
a pipeline, we decided on using a modular design. 
Each reduction approach described in Section~\ref{chap:minimization} consists 
of one or more components. 

These components can be shared and reused across multiple minimizing 
algorithms. 
This choice reduces the amount of copy-pasted code and increases its 
maintainability in the future. 
Depending on the technologies used in each app\-roach, the components form 
either a C++ application or a Python script. 
The latter is required for executing slicer implementations. 
This fact leads to inconsistent interface across multiple reduction 
approaches. 
For example, the user can launch the minimizing Delta debugging algorithm as 
a C++ console application. 
However, if he decides to execute the slicing-based approach, he will need 
to launch it as a Python script.

\begin{figure}[h]\centering
\includegraphics[scale=0.85]{components}
\caption{Communication between modules in three different pipelines:
Naive (orange), Delta (blue), Slicing (red and green).}
\label{img:components}
\end{figure}

Figure~\ref{img:components} shows a diagram of all available pipelines and 
their shared modules. 
Since some pipelines only add a preprocessing layer, they end up reusing 
the fundamental components of simpler pipelines.
Each module shown in the diagram will be described in the following sections.

\section{Shared components}\label{chap:components}

As was already mentioned, this project reuses several ideas and components 
in multiple approaches. 
One component does not necessarily translate to one idea. 
It often takes a couple of modules to capture that idea. 
The following subsections will describe each idea proposed in previous 
chapters. 
Each subsection will then outline components used by its particular idea.

\subsection*{Recursive visitor}

Some of the ideas in the following sections require us to traverse and alter 
the AST.
As explained in Section~\ref{chap:ast}, LibTooling allows us to traverse 
the AST using two methods. 
Splitting the code into code units could be done with either 
the \icode{RecursiveASTVisitor} or \icode{ASTMatchers}. 
We chose to implement the visitor interface to preserve more consistency 
across the project. 
Using a LibTooling visitor requires implementing three classes. 
Types of their implementation are described in detail in the following 
paragraphs.

\paragraph{Actions.} Each derivation of \icode{ASTFrontendAction} can have 
its own preprocessing and postprocessing steps.
Other than performing an action before and after a file is handled, it also 
creates a specific visitor.
In this project, we only utilize \icode{ASTFrontendAction} to create 
a consumer.
Moreover, the usage of actions tends to be module-specific. 
Usually, each component has its \emph{Actions.cpp} and \emph{Actions.h} files.
Further, some of the derivations use their custom factories.
By default, any \icode{ASTFrontendAction} can be created by calling 
the \icode{FrontendActionFactory::create()} method.
The method uses the default constructor and thus cannot provide any 
arguments for concrete \icode{ASTFrontendAction} implementations.
A workaround can be seen in \emph{Actions.cpp} files, most of which contain 
a custom factory.
The factory takes the necessary parameters, calls the desired constructor,
and passes them to a \icode{Consumer} instance in its \icode{create()} method.

\paragraph{Consumers.} This project distinguishes two types of 
\icode{Consumer} implementations. 
The first is a module-specific type. 
These \icode{Consumer} implementations contain high-level actions of 
a specific algorithm and are used for data transfer. 
For example, the \icode{VariantGeneratingConsumer} does not invoke any 
visitor instances. 
Instead, it keeps references to two more granular \icode{Consumer} objects. 
The \icode{VariantGeneratingConsumer} contains the main loop of the naive 
algorithm shown in Figure~\ref{chap:naive}. 
The pair of more granular consumers then carries out specific actions. 
The module-specific consumer also handles data exchanges between its more 
specific workers. 
One way of doing these exchanges is by keeping data structure instances 
inside the parent consumer. 
Another is by accessing the worker's internal states, either through their 
fields or getter methods. 
In summary, the module-specific consumer's job is to organize the granular 
parts of an algorithm, execute their actions, and collect their results.

The second \icode{Consumer} type is the shared consumer. 
These consumers are used for dispatching shared visitors, reporting their 
state, and collecting their data. 
The shared consumer responsible for code unit separation is 
the \icode{Dependency\-Mapping\-AST\-Consumer}. 
It dispatches a concrete visitor to the top-most node and offers getter 
methods to retrieve the visitor's data. 
This consumer also contains some logic in its 
\icode{HandleTranslationUnit} method. 
Other than dispatching the visitor, it also saves a visualization of 
the current AST. 
Other shared consumers might contain different logic in that method.


\paragraph{Visitors.} Visitors are once again separated into two types. 
A specific algorithm might use its unique visitor implementation to do 
an action such as slice extraction. 
This type of visitor is tied to a corresponding module-specific consumer. 
Similarly, shared visitors are also bound to their corresponding shared 
consumers. 
However, shared visitors are used for tasks that are used in both the naive 
and Delta algorithms. 
Those tasks are the node mapping and the variant generation. 
In our case, visitors contain the logic of a single iteration. 
Be it an iteration of the naive minimization or the Delta debugging; 
visitors are dispatched polynomially or even exponentially many times. 
The logic generally contains rules for handling each type of language 
construct. 
Two examples of such rules are:
\begin{itemize}
  \item Skipping the traversal of an implicit cast node of type 
  \icode{ImplicitCastExpr}, since removing it will not alter the source code.
  \item Creating a code unit for each statement that is not an expression.
\end{itemize}
The logic of a shared visitor is reused in multiple projects. 
This logic might include splitting the source code into code units. 
A module-specific visitor's logic is oriented to a particular problem, 
such as scanning the locations of statements in the code.

\subsection{Code units}\label{chap:codeunits}

Initially, a code unit was defined as any syntactically correct subset of 
the source code. 
It was then narrowed down to an atomic code unit - the smallest sensible 
syntactically correct source code element. 
Both the naive and the Delta algorithms use code units to perform their 
reductions. 
The goal is to split a given source code into partitions that can be later 
kept or removed. 
Those partitions would ideally not overlap and could be therefore 
independently removed.

We can split the input source code into code units by traversing its AST 
with a specified granularity. 
The idea is to analyze nodes based on hand-written rules. 
These rules specify which nodes are worth visiting and how to process them. 
While we are partitioning the source code, we might as well map the code's 
dependencies. 
This mapping can be done by extending our rules to perform specific lookup 
operations during the traversal. 
For example, the algorithm looks up all variable usages for a given variable 
declaration node. 
The results of those lookups are then kept in the dependency graph described 
in Section~\ref{chap:heuristics}.

\paragraph{Consumers.} The single consumer dedicated to code unit separation 
is implemented in the \icode{DependencyMappingASTConsumer} class. 
The consumer serves as a middle man for data transfers. 
Notably, it can retrieve data for the following structures:
\begin{itemize}
  \item Node mapping - this mapping is a lookup table that transitions 
  between a node's internal ID given by Clang and the current traversal 
  order number. 
  The traversal order number specifies the order in which the current node 
  is visited once the visitor is dispatched using 
  \icode{HandleTranslationUnit}. 
  The mapping is later transferred to other visitors to achieve the same 
  traversal order.
  \item Code units count - this property returns the total amount of code 
  units once their partitioning is complete.
  \item Dependency graph - is represented by a directional graph of nodes, 
  where each node is represented using its traversal order number. 
  Nodes in the graph might also contain additional information used for 
  debugging, such as their type and the code snippet they represent.
  \item Skipped nodes - this container serves as a list of nodes not worth 
  visiting. 
  These nodes might be too granular or solely implicit.
\end{itemize}
The consumer dispatches its concrete visitor, which carries most of 
the logic. 
The only standalone logic in the consumer is to dump a visualization of 
the source code based on the dependency graph.

\paragraph{Visitors.} What becomes a code unit is decided inside 
the \icode{MappingASTVisitor} class. 
The visitor traverses the AST in a postorder manner and stops at specified 
declarations, statements, and expressions. 
Each of these nodes must fulfill a predicate - their specific rule - to 
become a code unit. 
The node must not be a duplicate of an already visited node. 
It must also be a part of the main source file, i.e., not a part of a system 
header file. 
If the node satisfies all three requirements, it is considered a code unit. 
The node's ID mapping is then created, and the node is added to 
the dependency graph. 
On the other hand, if the node is not a valid code unit, it is added to 
the skipped nodes list. 
In that case, the next visitor will not consider visiting and manipulating 
the node when traversing the AST.

\subsection{Bitfield to variant transformation}\label{chap:variants}

Once the number of code units is known, we can create a bitfield of that 
size and generate representations of possible variants. 
The mechanism of generating bitfields was explained in 
Section~\ref{chap:naive}. 
This section describes the necessary parts of a technique that converts 
bitfields into source code variants. 
The idea is to flag AST nodes as code units and then traverse the AST again, 
removing the code unit nodes specified by a given bitfield. 
A single iteration of the variant-generating techniques produces one source 
code variant. 
The variant is created by starting with the original source code and 
gradually removing nodes. 
Each node has its index in the given bitfield. 
The index corresponds to the node's traversal order number. 
A node is removed if its corresponding bit is set to zero. 
To be more precise: the node is not removed from the AST. 
Instead, only its underlying source code is removed. 
The following paragraphs describe the code behind the removal.

\paragraph{Consumers.} All the necessary data for bitfield to source code 
conversion is acquired using the \icode{VariantPrintingASTConsumer}. 
This consumer dispatches its specific visitor and provides it with 
a bitfield. 
The visitor then removes snippets of the code based on that bitfield.
The consumer's job is to reset the visitor's state, launch it with 
the provided bitfield, and save its output into a specified file. 
The consumer can also access the visitor's internal state, namely its 
adjusted error line location. 
More details about this location can be found in the \emph{Visitors} section.

\paragraph{Visitors.} Similar to the code unit mapping issue, 
the \icode{Variant\-Printing\-AST\-Visitor} carries most of the conversion's 
logic.
It is provided with a bitfield, which it saves, and a \icode{Rewriter} 
instance, which it uses to generate the variant.
Section~\ref{chap:sts} describes the \icode{Rewriter} class in more detail.
Traversal of the source code is done in a postorder fashion to stay 
consistent with the previous visitor. 
The previous visitor shares its dependency graph and its list of skipped 
nodes.
This way, the \icode{VariantPrintingASTVisitor} can traverse the AST the way 
it was intended to. 

Each code unit, i.e., visited node, is removed if it fulfills specific 
requirements. 
First, the node's bit must be set to zero. 
Second, all its parent's bits must be set to one. 
The latter is a \icode{Rewriter} requirement. 
A parent node might represent a compound statement, and its children might 
be the individual statements inside the parent. 
If we were to remove the parent and one of its children, we would remove 
the same snippet twice. 
This catch would eventually lead to an error. 
The error can be avoided by only removing children while keeping their 
parents.

Removing code units tends to shift lines down. 
This phenomenon might lead to the statements on the original error line 
changing their location. 
The number of the error-inducing line must be adjusted. 
During the removal of each node, we extract its underlying code and analyze 
its presumed location. 
If the location comes before the original error-inducing line and 
the underlying code contains newline characters, we decrease 
the error-inducing line's number by the amount of found newline characters. 
At the end of the traversal, we are left with a correct line number for 
further validation, and all unwanted code units are removed. 
We are left with a \icode{Rewriter} instance whose buffer reflects 
the current variant. 
The visitor's iteration is done, and it is the consumer's job to save 
the contents of the buffer.

\subsection{Variant validation}\label{chap:validationimplementation}

Testing results on whether they are valid variants requires a standard 
interface, too.
Section~\ref{chap:verification} describes the steps in the process 
of validation.
The implementation contains three parts that are used in all approaches 
presented in this project.
Below is the description of compilation, analysis, and execution.

\paragraph{Compilation.} By calling the \icode{Compile} function in 
\emph{Helper.cpp}, one can invoke the Clang compiler driver.
The compiler has two goals.
Firstly, it filters out non-compilable and thus invalid variants.
Secondly, it prepares compilable variants for the execution stage.
The compilation can be invoked with a wide range of arguments.
In this case, it is provided with the \icode{-g} and \icode{-O0} options.
The former generates debug symbols for the executable, while the latter 
ensures reliable debugging by eliminating any compiler optimizations.
Compilation's output is printed to the standard output, and its exit status 
determines the function's return value.
If the compiler terminates with a valid exit code but does not create 
the binary, the function returns as if the compilation failed.
The binary is stored to a specified path, which by default is the same file 
path as the input source file.
The file extension is substituted with \icode{.exe}.

\paragraph{Static analysis.}
\change[inline]{TODO: Research and implement calls to the Clang static analyzer.}

\paragraph{Execution.} Compiled binaries need to be validated at runtime.
This way, we check whether the program results in the desired runtime error.
Programs are executed in the LLDB environment.
LLDB provides Python API, which allows invoking more or less all of 
the debugger's commands.
The API is also available from C++ using a scripting bridge.
SWIG processes function calls made from C++.
They then produce bindings to the Python API.
Thanks to the scripting bridge, every validation step is written in C++.
The \icode{ValidateResults} function creates a debugging environment for every 
executable.

The programs are then run in separate processes.
During the execution, events are broadcasted from the forked processes.
The stack trace is investigated whenever the program broadcasts a stopped 
state, indicating a thrown exception.
If the symbol's location on top of the stack trace is the same as the one of 
the desired error, the program is tagged as valid.
Otherwise, the execution continues.

\section{Naive reduction}\label{chap:naiveimplementation}

The algorithm for naive minimization is the foundation of this project. 
The approach and its heuristics were described in detail in 
Section~\ref{chap:naive}. 
The naive algorithm can guarantee minimality. Unfortunately, the same 
cannot be said for greedy approaches. 
Since this project focuses heavily on minimization, we dedicated 
a significant amount of time to improving the naive approach's 
implementation.

The reader has already seen most of this algorithm's implementation in 
Section~\ref{chap:components}. 
Nearly all shared components were created during the development of 
the naive approach. 
The implementation closely follows the algorithm shown in 
Figure~\ref{alg:naive}. 
As such, it required us to implement several vital mechanisms. 
First of all, the algorithm works with code units. 
As such, we had to create a way of splitting the code into several 
partitions. 
This lead to the implementation of a shared module that handles code units. 
That module has already been described in Section~\ref{chap:codeunits}. 
Second, we needed to convert bitfield representations into source code 
variants. 
This task required the development of a shared component that removes AST 
nodes based on a given bitfield. 
Details of this component can be found in Section~\ref{chap:variants}. 
The algorithm's last major requirement was the means to validate a variant. 
The implemented variant verification follows the steps described in 
Section~\ref{chap:verification}. 
Its implementation details as a shared module can be found in 
Section~\ref{chap:validationimplementation}.
The parts described above are not specific to the naive approach. 
Instead, they are used in the implementation of every suggested approach. 

However, the naive algorithm also uses its specific classes and functions. 
Two distinguishing factors are the bitfield manipulation and the validation 
of the dependency graph.

The naive algorithm works by iterating over all possible bitfield variants. 
Typically, the bitfield would be represented using an unsigned binary 
number. 
To~make working with the bitfield easier, we opted to represent it using 
a \icode{std::\-vector<\-bool>}. 
The vector is a good choice since its boolean variant uses 
a memory-efficient implementation in which each element only takes up 
a single bit. 
Working with the vector is also significantly more straightforward than 
with other representations. 
Possible bitfield variants are created by continuously incrementing 
the bitfield. 

However, the vector representation lacks the increment operation. 
We implemented it by flipping the first bit of the vector and keeping 
note of the carry-over bit. 
The implemented function's name is simply \icode{Increment}. 
As long as there is a carry-over bit, we keep flipping the consecutive 
indexes of the vector. 
Incrementing a bitfield where all bits are set to one leads to an overflow. 
While generating variants, we can prevent the overflow from happening by 
checking the \icode{IsFull} function. 
The function decides whether a given bitfield has all its bits set to one. 
The last used bitfield function is the \icode{IsValid} check. 
It determines whether a given bitfield might correspond to a valid source 
code variant. 

This function is also the first to utilize the dependency graph for its 
heuristic purposes. 
The function iterates over all bits in the bitfield. 
Each bit has to fulfill the following requirements. 
First, if the bit is set to zero, so must all of its children. 
This rule translates to ruling out both syntactically and semantically 
invalid variants. 
Second, if the bit represents a node in the criterion, i.e., 
the error-inducing location, it cannot be removed. 
This requirement preserves the failure-inducing line, ruling out pointless 
variants. 

The dependency graph validation is straightforward. 
Each node in the graph is represented using its traversal order number.
The exact number corresponds to the node's index in the bitfield. 
When validating all children of a particular node, we simply traverse 
a container of indices, checking whether the bitfield contains the desired 
bit value on each index.
The \icode{IsValid} function also calculates the size metric suggested in 
Section~\ref{chap:heuristics}. 
Each node has its presumed size in characters. 
The total size of the variant is calculated during the dependency graph 
validation. 
This way, we get an approximation of how large the source code variant is 
before generating it. 
The function returns a ratio of the size of the variant compared to 
the original size. 

This ratio serves two purposes. 
Firstly, the user can set their desired maximum ratio. 
If the user assumes the minimal variant is at most a quarter of 
the original size, they can set their target ratio to $0.25$. 
This assumption reduces the search space to only those variants whose 
presumed size is at most a quarter of the original size. 
Secondly, the implementation uses a sort of iterative deepening search. 
For a given number $k$, all valid bitfield variants are separated into $k$ 
bins. 
These bins represent parts of the interval from zero to the target ratio 
set by the user. 
The interval gets divided into $k$ evenly-sized parts, which then dictate 
the order in which the algorithm searches. 
First, the algorithm generates and validates all variants in the bin that 
contains the presumably smallest variants. 
If no valid result is found, the search continues with the bin containing 
the next smallest variants. 
Ideally, the search finishes faster than usual. 
This case would save time otherwise spent on generating all variants. 
In the worst case, the algorithm will generate and validate all possible 
variants, just like it did originally. 
Figure~\ref{alg:bins} shows the pseudocode of the bin heuristic.

\begin{figure}[h]
	\hrule height.8pt depth0pt \kern2pt
	\textbf{Input:} \\
	\hspace*{\algorithmicindent} $B \ldots$ a set of bins - containers of bitfield. \\
	\hspace*{\algorithmicindent} $L \ldots$ location of the error. \\
	\hspace*{\algorithmicindent} $A \ldots$ the input program's arguments. \\
	\textbf{Output:} The reduced source code. 
	\hrule height.8pt depth0pt \kern2pt
	\begin{algorithmic}[1]
		\ForAll{$Bin \in B$}
			\ForAll{$Variant \in B$}
				\If{IsValid($Variant$, $L$, $A$)}
					\Return $V$.
				\EndIf
			\EndFor
		\EndFor
	\end{algorithmic} 
	\hrule height.8pt depth0pt \kern2pt
	\caption{Binning search.} 
	\label{alg:bins}
\end{figure}

In the ideal world, we would know the exact size of a variant ahead of time. 
Knowing this, we could sort the variants before generating them, resulting 
in the best possible outcome. 
However, the presumed variant size is just an approximation. 
Clang's information on the underlying source code is not perfect. 
Using bins decreases the chances of classifying a non-minimal variant as 
the optimal result.

All the mentioned functions can be in the \emph{Common/\-src/\-Helper.cpp} 
file. 
The dependency graph resides in its file, 
\emph{Common/\-include/\-DependencyGraph.h}.

The mentioned functions are called from the algorithm's main loop. 
In our implementation, the loop is in the \icode{Handle\-Translation\-Unit} 
function of the \icode{Vari\-ant\-Generating\-Consumer} class. 
This consumer is specific to the naive reduction problem. 
It does not dispatch any visitors; it only governs two other consumer 
objects. 
Those objects are the instances of 
the \icode{Dependency\-Mapping\-AST\-Consumer} and 
the \icode{VariantPrintingASTConsumer} classes. 
The former partitions the input into code units, while the latter converts 
bitfields into source code variants. 
The body of \icode{HandleTranslationUnit} manages all data exchanges 
between the two consumers. 
It also contains the bitfield iteration logic. 
The consumer handles generating the variants, while their validation is 
invoked from the program's \icode{main} function. 

The consumer can be constructed using a custom factory. 
The \icode{Variant\-Generating\-Frontend\-Action\-Factory} function takes 
a reference to a \icode{GlobalCon\-text} object and later passes it to 
the visitor. 
The context is used for looking up the original input and the current state of the algorithm.

The problem-specific consumer can be found in 
the \emph{NaiveReduction/\-src/\-Consumers.cpp} file. 
The action that invokes the consumer is 
in \emph{NaiveReduction/\-src/\-Actions.cpp} and 
\emph{NaiveReduction/\-include/\-Actions.h}. 
The \icode{GlobalContext} resides in its own file under 
\emph{Common/\-include/\-Context.h}.

\section{Delta debugging}\label{chap:deltaimplementation}

Implementing the minimizing Delta debugging algorithm took significantly 
less time than expected. 
The implementation of the naive approach laid down a good enough foundation 
to make implementing Delta easy. 
The algorithm reuses existing components for code unit partitioning, 
bitfield to source code conversion, and validation. 
This section describes how it does so.

The minimizing Delta debugging algorithm is typically implemented using 
a text-modifying approach. 
The input test case could be split into $n$ partitions based on many factors.
One implementation might split the input on an exact number of characters, 
while another might do so on a specific number of lines. 
Text-oriented Delta debugging is not the only way of approaching Delta's 
implementation. 
Hierarchical Delta debugging (HDD) presented by Misherghi and 
Su~\citep{Misherghi06} handles the minimization using an AST. 
Implementing HDD would be ideal for this project since it we are working 
with the AST already. 
However, due to work already done on the naive approach in 
Section~\ref{chap:naiveimplementation}, we chose to implement a modified 
version of the minimizing Delta algorithm. 

Our implementation performs the following steps during each iteration:
\begin{enumerate}
  \item Input source code is partitioned into code units in the same manner 
  as in the naive approach, using the shared component described in 
  Section~\ref{chap:codeunits}.
  \item Bitfields are created in order to represent possible Delta 
  partitions and their complements. 
  The Delta algorithm splits the current test case into $n$ partitions and 
  their complements in each iteration. 
  This split translates to code units being assigned into specific variants. 
  Bitfields corresponds to one of 
  the $\alpha_1,\dots,\alpha_n,\beta_1,\dots,\beta_n$ variants. 
  The bitfield's elements are set to one if they represent snippets in their 
  variant.
  \item The bitfields are iterated over according to the algorithm in 
  Figure~\ref{alg:dd}. 
  Each bitfield is converted into source code using the component shown in 
  Section~\ref{chap:variants}. 
  The code is then validated. 
  If it causes the desired runtime error, it is regarded as the test case 
  for the next iteration. 
  The validation uses the module described in 
  Section~\ref{chap:validationimplementation}.
\end{enumerate}
The described approach is nearly equivalent to removing lines of the source 
code. 
However, it does leave some room for structure-based heuristics for future 
work.

The algorithm's main loop is inside the program's \icode{main} function. 
Each iteration invokes a custom visitor that handles it. 
Clang parses the current test case, and a \icode{Delta\-Debugging\-Consumer} 
instance performs its logic on the generated AST. 
The logic consists of the three steps shown in the previous paragraph: 
partitioning, generating variants, and validating. 
The iteration's result is then passed by reference back to the \icode{main} 
function. 

Similar to implementing the naive approach, the problem-specific consumer is 
invoked from a custom \icode{ASTFrontendAction}. 
The \icode{Delta\-Debugging\-Frontend\-Action\-Factory} function supplies 
data to the consumer. 
The data contains the following:
\begin{itemize}
  \item Iteration number - this number is used for correct file naming.
  \item Partition count - this number represents the number of even-sized 
  partitions into which the input should be split.
  \item Iteration result - the result is passed as a reference to 
  an \icode{enum}, which the consumer then specifies.
  \item Global context - analogically to the naive reduction's implementation, 
  the context serves to lookup information concerning the algorithm's input 
  and state.
\end{itemize}
The mentioned classes can be found in the following files: 
\begin{itemize}
  \item \emph{DeltaReduction/include/Consumers.h} 
  \item \emph{DeltaReduction/include/Actions.h} 
  \item \emph{DeltaReduction/src/Actions.cpp}
  \item \emph{Common/include/Context.h}
\end{itemize}

\section{External code}

As was already mentioned at the beginning of this chapter, implementing 
a slicer is out of this project's scope. 
We have turned to existing implementations of static and dynamic slicers. 
We know that these implementations will be more reliable and accurate. 
Below are the details concerning the two chosen slicer implementations. 
We were searching for LLVM-based slicers in the hope of running them natively 
during the reduction. 
However, due to compatibility reasons, the slicers cannot be easily shipped 
and executed with the current implementation of the rest of the project. 
Therefore, we run both slicers in their available Docker containers.

\paragraph{DG.} We chose DG as it is the best LLVM-based static slicer found. 
It slices LLVM bitcode and therefore requires the input in the compiled form. 
This requirement imposes the first obstacle - compiling the given source file 
to the LLVM intermediate representation. 
This task is not difficult for programs with a simple compilation command; 
however, handling large projects might be difficult. 
DG supports multiple criteria. 
One can slice w.r.t. either a function call or a variable on a given line. 
The variable criterion only works correctly for bitcode compiled with 
debugging symbols. 
DG also handles secondary slicing criteria, which are not required for this 
project. 

The output of DG is the sliced bitcode. 
The slice can be analyzed either by disassembling the bitcode or converting 
it to a list of line numbers using provided scripts. 
Nonetheless, we end up with a usable slice representation.

It should be noted that DG does not support C++ by design. 
Slicing its bitcode might not be an issue. 
However, there is a chance of encountering unsupported instruction.

DG is available on GitHub. 
The repository can be found at \url{https://github.com/mchalupa/dg}.

\paragraph{Giri.}

Giri is an LLVM-based backward dynamic slicer. 
The project started its development during the Google Summer of Code in 2013, 
and it has not received many updates since. 
It runs on a deprecated version of LLVM that cannot be installed on modern 
systems. 
In this project, we attempted to port Giri to a newer version of LLVM but 
ultimately failed. 
While most changes to Giri would have been trivial, others required much 
effort in reimplementing legacy constructs. 
Our update was therefore dropped.

One can use Giri via a \icode{Makefile}. 
This unconventional approach hides the standard interface of the slicer. 
Instead, the user specifies their desired criterion in a text file. 
Criteria must specify a path to the sliced source file and a line number. 
The text file is then passed to the \icode{make} command, and a prepared 
\icode{Makefile} is executed. 
The user can also specify a concrete instruction's number instead of its 
line number. 
However, this feature is not used in our project.

Similar to DG, Giri also produces a list of line numbers. 
This slice representation then has to be extracted from the original source 
code.

Giri's repository can be found on GitHub on the following address: 
\url{https://github.com/liuml07/giri}.

\section{Systematic approach}

The systematic approach is a pipeline comprised of multiple steps. 
The steps are represented by components, some of which are shared with 
the previous implementation. 
The entire pipeline consists of a variable extraction tool, static and 
dynamic slicers, a slice unification tool, a slice extraction tool, 
the minimizing Delta debugging algorithm, and the naive approach. 
The motivation behind these steps and an overview of the algorithm can be 
found in Section~\ref{chap:systematic}.

This approach uses external code that cannot be trivially added to 
the project.
Therefore, it is launched and operated differently.
The external code forms two modules - containers. 
The static slicer is accessed via the DG container and the dynamic slicer 
via the Giri container. 
The unifying base is a Python script that invokes all necessary components.
The script uses Docker API to launch the DG container.
It also maps input and output directories to that container in order to send 
and retrieve data.
The container's launch command invokes the slicer to process the given input 
and store it in the given output directory.
The Giri container is launched analogically.

Both slicers produce a list of lines that represent the slices. 
The slicers are also run repeatedly, once for each variable on a given line. 
These variables are extracted using a \icode{VariableExtractor} tool. 
This module uses LibTooling's ASTMatchers to search for variable usages on 
a given line. 
The names of the symbols are then dumped into an output file. 
Slicers are executed with different criteria, and the criteria are taken 
from that file.
These runs generate multiple slices, which must be unified into a single one. 
The unification is simple. 
The slice is represented by a file containing a list of line numbers. 
The multiple generated files are merged, removing any duplicates. 
The result is a single file containing a single enhanced slice. 

This output needs to be processed further. 
The base script invokes the \icode{SliceExtractor} component. 
This module uses LibTooling's recursive AST visitor to access statements on 
given lines and preserve them. 
The program transforms a source file into the desired slice based on 
the given list of lines. 
First, the tool creates a container of preserved lines. 
Initially, the container consists of the lines given through the input. 
Later, the container expands by adding other previously poorly-mapped lines. 
For example, if a \icode{for} loop stretches over several lines and 
the slicer result only contains its first line, we must include those other 
lines as well. 
We do this by traversing the AST and inspecting nodes on given lines. 
If we encounter a node whose underlying code stretches over several lines, 
we include those lines in the container. 
The input source file is then opened and read, and the lines present in 
the container are preserved. 

Once the \icode{SliceExtractor} produces the desired source file, the file 
is considered the respective slicer's output.
This way, each step of the algorithm results in a valid source file.

Once the slicer preprocessing is done, the pipeline executes the implemented 
Delta debugging algorithm and finds a local minimum. 
The output is a source file which then serves as the input for the naive 
approach.
The Python script executes the naive algorithm, which then produces the 
desired results.
The implementation of both the naive reduction and the minimizing Delta 
debugging algorithm can be found in Section~\ref{chap:naiveimplementation} 
and Section~\ref{chap:deltaimplementation}, respectively.

The classes and functions used for implementing 
the \icode{VariableExtractor} and the \icode{SliceExtractor} modules can be 
found in their respective directories:
\begin{itemize}
  \item \emph{VariableExtractor/include}
  \item \emph{VariableExtractor/src}
  \item \emph{SliceExtractor/include}
  \item \emph{SliceExtractor/src}
\end{itemize}
The listed directories contain the problem-specific code. 
However, the tools also utilize code from shared components to carry out 
some repetitive tasks.