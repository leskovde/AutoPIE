\chapter{Automated debugging techniques}

\change[inline]{TODO: Link relevant literature from Slicing of LLVM bitcode (muni.cz) and Bobox Runtime Optimization (cuni.cz)}

Debugging can be described as the process of analyzing erroneous code to find 
the cause of those errors.
While most developers see debugging as a manual chore, there were numerous 
attempts~ at automating at least some parts of it during the last few decades. 
The rise in popularity of program analysis resulted in automated error checks 
for popular programming languages. 

While these checks mostly cover only specific 
cases~of potential bugs, such as out-of-range array indexing, they have proven 
themselves as a useful tool for the developer. In the context of this work, 
such checks provide a helping hand at a low cost when minimizing a program. 

The following sub-chapters will talk about the techniques behind such checks 
and how they deal with automated debugging.

\section{Delta debugging}

Delta debugging is an iterative approach described by Zeller in 1999. 
It does not perform any static analysis of the debugged program, as it 
is~not meant to find failures in the code. 

Delta debugging instead intends to 
minimize the debugged program's incorrect input to isolate the input's 
failure-inducing part. 
Therefore, it requires the program in question and the specific input 
and the expected output. 
In other words, Delta debugging requires a set of test cases, which attempts to 
minimize and isolate the failure-inducing input. Minimality is defined as follows.

\begin{defn}[Test case]\label{def02:1}
  Let $c_\mathcal{F}$ be a set of all changes $\delta_1,\dots,\delta_n$ 
  between a passing program run $r_\mathcal{P}$ and a failing program run
  $r_\mathcal{F}$ such that 
  \begin{align}
	r_\mathcal{F} = (\delta_1(\delta_2(\dots(\delta_n(r_\mathcal{P}))))). \nonumber 
  \end{align}
  We call a subset $c \subseteq c_\mathcal{F}$ a \emph{test case}.
\end{defn}

\begin{defn}[Global minimum]\label{def02:2}
  A test case $c \subset c_\mathcal{F}$ is called a \emph{global minimum}
  of $c_\mathcal{F}$ if $\forall c_i \subseteq c_\mathcal{F}:
  (|c_i| < |c| \implies c_i$ does not cause the program to fail.$)$
\end{defn}

Global minimum can be interpreted as the smallest set of changes able to
make the program fail.

\begin{defn}[Local minimum]\label{def02:3}
  A test case $c \subset c_\mathcal{F}$ is called a \emph{local minimum}
  of $c_\mathcal{F}$ if $\forall c_i \subseteq c:
  (c_i$ does not cause the program to fail.$)$
\end{defn}

\begin{defn}[$n$-minimality]\label{def02:4}
  A test case $c \subset c_\mathcal{F}$ is \emph{$n$-minimal}
  if $\forall c_i \subseteq c:
  (|c| - |c_i| \leq n \implies c_i$ does not cause the program to fail.$)$
\end{defn}

The minimizing Delta debugging algorithm attempts to find a 1-minimal test case.

Delta debugging seems to bet on the premise that large-scale applications are written
with automated testing in mind. On the same note, it is the recommended practice to
develop programs while at the same time dedicating resources to write tests for that
program.

The defined minimality can be used to construct the minimizing algorithm. 
However, the delta debugging algorithm can be easily and more comprehensively explained 
without the definition as well.

\begin{algorithm}
	\caption{Minimizing Delta Debugging Algorithm} 
	\begin{algorithmic}[1]
		\State $n \leftarrow 2$
		\State Split a string $S$ into $\alpha_1,\dots,\alpha_n$ of equal size.
		\State For each $\alpha_i$, calculate its complement $\beta_i$.
		\State Run tests on $\alpha_1,\dots,\alpha_n,\beta_1,\dots,\beta_n$.
		\If{all tests passed}
			\State $n \leftarrow 2*n$
			\If{$n > |\sigma|$}
				\Return the most recent failure causing substring.
			\Else
				\State goto (2).
			\EndIf
		\ElsIf{$\alpha_i failed$}
			\State $n \leftarrow 2$.
			\State $\sigma \leftarrow \alpha_i$.
			\If{$|\sigma| == 1$}
				\Return $\sigma$.
			\Else
				\State goto (2).
			\EndIf
		\Else
			\Comment $\beta_i$ failed.
			\State $\sigma \leftarrow \beta_i$.
			\State $n \leftarrow n - 1$.
			\State goto (2).
		\EndIf
	\end{algorithmic} 
\end{algorithm}

Additionally, minimizing is not the only approach Delta debugging suggests.
A more sophisticated one is isolation. Minimization can be described as removing parts
while the failure persists, which means that the output changes are only made in failing
iterations.
Isolation extends this by adding failure-inducing differences while the program passes tests.
This addition results in changes in both the passing and failing iterations.

One can quickly transform the input minimalization of Delta debugging into either source
code minimalization or error isolation at both the compile-time and runtime.
This transformation can be achieved for the compile-time by first setting the input
as the debugged program's source code. 
Second, it is required to set the expected
output to either 'compiled' or 'failed to compile'. 
Finally, the input is fed into a compiler, for example, GCC, which produces
one of the two set outputs. 

The runtime variant only differs in two pointsâ€”first, changing the expected outputs. 
Second, changing the compiler to a compiler-debugger pipeline so that the source 
can be compiled and run.

\section{Static slicing}

The first introduced slicing method was static backward slicing. 
In 1984, Weiser defined a slice with respect to criterion C 
as a part of a program that potentially affects given variables in a given point. 

\begin{defn}[Static slicing criterion]\label{def02:5}
  Let $\mathcal{P}$ be a program consisting of program points 
  $P = p_1,\dots,p_n$ and variables $V = v_1,\dots,v_m$.
  Any pair $C = (p_i, V')$, such that $p_i \in P$, $V' \subseteq V$, and 
  $\forall v_i \in V': v_i$ is present in $p_i$, 
  is called a \emph{slicing criterion}.
\end{defn}

Slicing is the process of finding such a part of a program. 
Suggested approaches neglected any execution information and 
focused solely on observations made by analyzing the code.

\change[inline]{TODO: Convert the pseudocode to an easily readable
version (i.e. comparison with the non-sliced program).}

\begin{algorithm}
	\caption{Simple Branching Program} 
	\begin{algorithmic}[1]
		\State $x \leftarrow 1$
		\State $a \leftarrow$ read($a$)
		\For{$i = 1,2,\dots,C$}
			\State write($i$)
		\EndFor
		\If{$a$ mod $2 == 0$}
			\If{$a \neq 0$}
				\State $x \leftarrow -1 * x$
			\Else
				\State $x \leftarrow 0$
			\EndIf
		\Else
			\State $x \leftarrow x + 1$.
		\EndIf
		\State write($x$)
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Static Slice of the Simple Branching Program} 
	\begin{algorithmic}[1]
		\State $x \leftarrow 1$
		\State $a \leftarrow$ read($a$)
		\If{$a$ mod $2 == 0$}
			\If{$a \neq 0$}
				\State $x \leftarrow -1 * x$
			\Else
				\State $x \leftarrow 0$
			\EndIf
		\Else
			\State $x \leftarrow x + 1$.
		\EndIf
		\State write($x$)
	\end{algorithmic}
\end{algorithm}

Later that year, Ottenstein and Ottenstein restated the problem as a reachability
search in the program dependence graph (PDG).
PDG represents statements in the code as vertices and data and control
dependencies as oriented edges. 
Additionally, edges induce a partial ordering on the vertices. 
In order to preserve the semantics of the program, statements must be executed 
according to this ordering. 

Edges are, therefore, of two types. 
First, the control dependency edge specifies that an incoming vertex's 
execution depends on the outgoing one's execution. 
Second, the data flow dependence edge suggests that a variable appearing
in both the outgoing and incoming edge share a variable,
the value of which depends on the order of the vertices execution.

Once the PDG is built, slices can be extracted in linear time 
with respect to the number of vertices.

\change[inline]{TODO: Show how PDG is sliced, from 2.2 Slicing of LLVM bitcode (muni.cz)}

However, one can find many potential issues and obstacles when performing 
data flow analysis. 
Omitting the interprocedural slicing, as it is not relevant in this paper's
context, one is left with pointers and unstructured control flow.
While the latter is seldomly used in single-threaded modern programming, 
the same cannot be said about the former. 

Pointers require us to extend the syntactic data flow analysis 
into a pointer or points-to analysis, which should be performed first. 
It is necessary to keep track of where pointers may point to (or must point to,
in case their address is not reassigned) during the execution. 
From this knowledge, other data flow edges must be created or
changed to accommodate the fact when the outgoing vertex mayhap writes
into a memory location possibly used by the incoming vertex. 

The analogical approach is then used for control dependency analysis since 
pointers might alter control flow as well. 
This change to control flow happens, namely when functions are called using 
function pointers.

The main advantage of static slicing is that it does not require
any runtime information. 
As program execution can be expensive both time-wise and resource-wise, 
static slicing offers program comprehension at a low cost. 
Because static slicing discovers program statements that can affect 
certain variables, it can remove dead code and be used for program segmentation. 

Furthermore, static slicing is used for testing software quality, maintenance, 
and test, all of which are relevant to this project.

\section{Dynamic slicing}

While the idea of building a program slice prevails, dynamic slicing 
drastically differs from static slicing in terms of input and the way
it is processed. 

In 1988, Korel and Laski described a slicing approach that took into 
consideration information regarding a program's concrete execution. 
As opposed to static slicing, which builds a slice for any execution, 
dynamic slicing builds a slice for a given execution of a program. 
Using information available during a run of the program 
results in a typically much smaller slice.

\change[inline]{TODO: Convert the pseudocode to an easily readable
version (i.e. comparison with the non-sliced program).}

\begin{algorithm}
	\caption{Dynamic Slice of the Simple Branching Program (for $a = 2$)} 
	\begin{algorithmic}[1]
		\State $x \leftarrow 1$
		\State $a \leftarrow$ read($a$)
		\State $x \leftarrow 0$
		\State write($x$)
	\end{algorithmic}
\end{algorithm}

This decrease in size is mainly due to removing unnecessary 
branching of control statements and unexecuted statements in general. 
The slicing criterion now contains a set of the program's 
arguments in addition to the previous information. 
The location of the criterion's statement is also specified to avoid 
vagueness in the execution history. 

The criterion is therefore defined as follows.

\begin{defn}[Dynamic slicing criterion]\label{def02:5}
  Let $\mathcal{H} = (s_{x1},\dots,s_{xn})$ be an execution history of a program 
  $\mathcal{P} = (\{s_1,\dots,s_m\}, V)$, where $s_i$ denotes a statement
  and V is a set of variables $v_1,\dots,v_k$.
  Any triple $C = (h_i, V', \{a_1,\dots,a_j\})$, such that $h_i \in \mathcal{H}$,
  $V' \subseteq V$, $\forall v_i \in V': v_i$ is present in $h_i$,
  and $\{a_1,\dots,a_j\}$ is the input of the program,
  is called a \emph{slicing criterion}.
\end{defn}

Since dynamic slicing requires the user to run the program, 
it is typically used in cases where the execution with a fixed 
input happens regardless. Such cases include debugging and testing. 
For debugging, dynamic slices must reflect the subsequent restriction: 
a program and its slices must follow the same execution paths.

\section{Summary}

While the described program minimizing and debugging approaches have been 
formulated more than two decades ago, there have not been nearly enough 
successful attempts at implementing them. 

With each approach having its clear positives and negatives, 
it would be interesting to see how they handle program minimization. 
When cleverly used, a combination of these methods might 
result in a reasonably fast and inexpensive algorithm 
for the reduction of program size.